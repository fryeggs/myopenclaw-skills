#!/usr/bin/env python3
"""
Fetch AI-related Reddit posts via RSS feeds.
Outputs Chinese title, summary and URL.

Usage:
    python3 fetch_reddit.py [--sort hot|new|top] [--limit N] [--subreddits sub1,sub2,...]

Examples:
    python3 fetch_reddit.py --limit 10
    python3 fetch_reddit.py --subreddits LocalLLaMA,ClaudeAI --limit 5
"""

import argparse
import re
import sys
import urllib.request
import html
import xml.etree.ElementTree as ET
from typing import List, Dict
from datetime import datetime
import time

# Default AI subreddits
DEFAULT_SUBREDDITS = [
    # Core LLM communities
    "LocalLLaMA",
    "ollama",
    # Major AI providers
    "Anthropic",
    "ClaudeAI",
    "ClaudeCode",
    "OpenAI",
    "ChatGPT",
    "DeepSeek",
    "GeminiAI",
    "google_antigravity",
    "kimi",
    # AI coding tools
    "cursor",
    "kiroIDE",
    # OpenClaw ecosystem
    "openclaw",
    "clawdbot",
    "moltbot",
    # Other AI tools
    "notebooklm",
    "LangChain",
    "nanobanana",
    # Research & general
    "MachineLearning",
    "singularity",
]

# Subreddit Chinese descriptions
SUBREDDIT_INFO = {
    # Core LLM
    "localllama": ("LocalLLaMA", "ğŸ¦™ æœ¬åœ°å¤§æ¨¡å‹ç¤¾åŒº"),
    "ollama": ("ollama", "ğŸ¦™ Ollamaæœ¬åœ°æ¨¡å‹"),
    # Major AI providers
    "anthropic": ("Anthropic", "ğŸ›ï¸ Anthropicå®˜æ–¹"),
    "claudeai": ("ClaudeAI", "ğŸ¤– Claudeè®¨è®º"),
    "claudecode": ("ClaudeCode", "ğŸ’» Claude Code"),
    "openai": ("OpenAI", "ğŸ”¬ OpenAI"),
    "chatgpt": ("ChatGPT", "ğŸ’¬ ChatGPTè®¨è®º"),
    "deepseek": ("DeepSeek", "ğŸ” DeepSeek"),
    "geminiai": ("GeminiAI", "ğŸ’ Gemini AI"),
    "google_antigravity": ("google_antigravity", "ğŸš€ Google Antigravity"),
    "kimi": ("kimi", "ğŸŒ™ Kimi/æœˆä¹‹æš—é¢"),
    # AI coding tools
    "cursor": ("cursor", "ğŸ–±ï¸ Cursor IDE"),
    "kiroide": ("kiroIDE", "âŒ¨ï¸ Kiro IDE"),
    # OpenClaw ecosystem
    "openclaw": ("openclaw", "ğŸ¦ OpenClaw"),
    "clawdbot": ("clawdbot", "ğŸ¤– Clawdbot"),
    "moltbot": ("moltbot", "ğŸ¦ Moltbot"),
    # Other AI tools
    "notebooklm": ("notebooklm", "ğŸ““ NotebookLM"),
    "langchain": ("LangChain", "ğŸ”— LangChain"),
    "nanobanana": ("nanobanana", "ğŸŒ Nanobanana"),
    # Research & general
    "machinelearning": ("MachineLearning", "ğŸ“Š æœºå™¨å­¦ä¹ ç ”ç©¶"),
    "artificial": ("artificial", "ğŸ§  AIç»¼åˆ"),
    "singularity": ("singularity", "ğŸš€ AGI/å¥‡ç‚¹"),
    "stablediffusion": ("StableDiffusion", "ğŸ¨ AIå›¾åƒç”Ÿæˆ"),
}

# Key term translations (for title and content)
TERM_TRANSLATIONS = {
    # Actions
    "release": "å‘å¸ƒ", "released": "å‘å¸ƒ", "launching": "å‘å¸ƒ",
    "announce": "å®£å¸ƒ", "announcing": "å®£å¸ƒ", "introducing": "æ¨å‡º",
    "update": "æ›´æ–°", "updates": "æ›´æ–°", "upgrade": "å‡çº§",
    "built": "æ„å»º", "made": "åˆ¶ä½œ", "created": "åˆ›å»º",
    "support": "æ”¯æŒ", "supports": "æ”¯æŒ",
    # Technical terms
    "model": "æ¨¡å‹", "models": "æ¨¡å‹",
    "benchmark": "åŸºå‡†æµ‹è¯•", "benchmarks": "åŸºå‡†æµ‹è¯•",
    "fine-tuning": "å¾®è°ƒ", "fine tuning": "å¾®è°ƒ", "finetuning": "å¾®è°ƒ",
    "quantization": "é‡åŒ–", "inference": "æ¨ç†",
    "context": "ä¸Šä¸‹æ–‡", "token": "ä»¤ç‰Œ", "tokens": "ä»¤ç‰Œ",
    "GPU": "æ˜¾å¡", "VRAM": "æ˜¾å­˜",
    "open source": "å¼€æº", "open-source": "å¼€æº", "opensource": "å¼€æº",
    "local": "æœ¬åœ°", "locally": "æœ¬åœ°",
    "parameter": "å‚æ•°", "parameters": "å‚æ•°",
    "training": "è®­ç»ƒ", "reasoning": "æ¨ç†èƒ½åŠ›",
    "coding": "ç¼–ç¨‹", "code": "ä»£ç ",
    "agent": "æ™ºèƒ½ä½“", "agents": "æ™ºèƒ½ä½“", "agentic": "æ™ºèƒ½ä½“",
    "MoE": "æ··åˆä¸“å®¶", "flash": "æé€Ÿç‰ˆ",
    # Common phrases
    "how to": "å¦‚ä½•", "why": "ä¸ºä»€ä¹ˆ", "what": "ä»€ä¹ˆ",
    "best": "æœ€ä½³", "new": "æ–°", "free": "å…è´¹",
    "faster": "æ›´å¿«", "better": "æ›´å¥½", "vs": "å¯¹æ¯”",
    "comparison": "å¯¹æ¯”", "guide": "æŒ‡å—", "tutorial": "æ•™ç¨‹",
    "tips": "æŠ€å·§", "help": "å¸®åŠ©",
    "issue": "é—®é¢˜", "issues": "é—®é¢˜", "bug": "é”™è¯¯",
    "bugs": "é”™è¯¯", "error": "é”™è¯¯", "fix": "ä¿®å¤", "fixed": "å·²ä¿®å¤",
    "plugin": "æ’ä»¶", "plugins": "æ’ä»¶", "tool": "tools", "tools": "å·¥å…·",
    "runtime": "è¿è¡Œæ—¶", "server": "æœåŠ¡å™¨", "servers": "æœåŠ¡å™¨",
    "api": "æ¥å£", "limit": "é™åˆ¶", "limits": "é™åˆ¶",
    "performance": "æ€§èƒ½", "speed": "é€Ÿåº¦", "memory": "å†…å­˜",
    "hallucination": "å¹»è§‰", "hallucinations": "å¹»è§‰",
    "prompt": "æç¤ºè¯", "prompts": "æç¤ºè¯", "engineering": "å·¥ç¨‹",
    "feedback": "åé¦ˆ", "community": "ç¤¾åŒº", "discussion": "è®¨è®º",
    "megathread": "è®¨è®ºå¸–", "AMA": "é—®ç­”",
    "lawsuit": "è¯‰è®¼", "sue": "èµ·è¯‰", "billion": "åäº¿", "million": "ç™¾ä¸‡",
}

# Title translation patterns
TITLE_PATTERNS = [
    (r"(?i)^released[:\s]", "å‘å¸ƒï¼š"),
    (r"(?i)^announcing[:\s]", "å®£å¸ƒï¼š"),
    (r"(?i)^introducing[:\s]", "æ¨å‡ºï¼š"),
    (r"(?i)^how to\s", "å¦‚ä½•"),
    (r"(?i)^why\s", "ä¸ºä»€ä¹ˆ"),
    (r"(?i)^what\s", "ä»€ä¹ˆæ˜¯"),
    (r"(?i)\bAMA\b", "é—®ç­”"),
    (r"(?i)\bmegathread\b", "è®¨è®ºå¸–"),
    (r"(?i)\bopen[- ]?source\b", "å¼€æº"),
    (r"(?i)\bhallucination[s]?\b", "å¹»è§‰"),
    (r"(?i)\bplugin[s]?\b", "æ’ä»¶"),
]


def translate_title(title: str) -> str:
    """Create Chinese title translation using pattern matching."""
    zh_title = title
    for pattern, replacement in TITLE_PATTERNS:
        zh_title = re.sub(pattern, replacement, zh_title)
    for en, zh in sorted(TERM_TRANSLATIONS.items(), key=lambda x: -len(x[0])):
        zh_title = re.sub(r'(?i)\b' + re.escape(en) + r'\b', zh, zh_title)
    return zh_title


def summarize_content(title: str, content: str) -> str:
    """Generate brief Chinese summary based on content detection."""
    full_text = (title + " " + content).lower()
    
    if "ama" in full_text or "ask me anything" in full_text:
        return "é—®ç­”è®¨è®ºï¼Œå¼€å‘è€…/å›¢é˜Ÿå›ç­”ç¤¾åŒºé—®é¢˜"
    elif any(x in full_text for x in ["release", "launching", "announcing"]):
        return "æ–°ç‰ˆæœ¬/äº§å“å‘å¸ƒå…¬å‘Š"
    elif "megathread" in full_text:
        return "ç¤¾åŒºé›†ä¸­è®¨è®ºå¸–"
    elif any(x in full_text for x in ["bug", "issue", "error", "fix"]):
        return "é—®é¢˜æŠ¥å‘Š/ä¿®å¤è®¨è®º"
    elif any(x in full_text for x in ["how to", "guide", "tutorial"]):
        return "æ•™ç¨‹/æŒ‡å—"
    elif any(x in full_text for x in ["vs", "comparison", "better"]):
        return "å¯¹æ¯”/è¯„æµ‹è®¨è®º"
    elif any(x in full_text for x in ["tool", "built", "made"]):
        return "å·¥å…·/é¡¹ç›®åˆ†äº«"
    elif "lawsuit" in full_text or "sue" in full_text:
        return "æ³•å¾‹/è¯‰è®¼ç›¸å…³æ–°é—»"
    elif "feedback" in full_text:
        return "ç”¨æˆ·åé¦ˆæ”¶é›†"
    elif "update" in full_text:
        return "æ›´æ–°/æ”¹è¿›å…¬å‘Š"
    elif "limit" in full_text or "quota" in full_text:
        return "ä½¿ç”¨é™åˆ¶ç›¸å…³è®¨è®º"
    elif "hallucination" in full_text:
        return "AIå¹»è§‰é—®é¢˜è®¨è®º"
    elif "plugin" in full_text:
        return "æ’ä»¶åŠŸèƒ½æ›´æ–°"
    elif any(x in full_text for x in ["agent", "agentic"]):
        return "AIæ™ºèƒ½ä½“ç›¸å…³"
    elif any(x in full_text for x in ["code", "coding"]):
        return "ç¼–ç¨‹/ä»£ç ç›¸å…³"
    elif "model" in full_text:
        return "æ¨¡å‹è®¨è®º"
    
    return content[:60] + "..." if content and len(content) > 10 else "ç¤¾åŒºè®¨è®º"


def get_subreddit_desc(subreddit: str) -> str:
    """Get Chinese description for subreddit."""
    key = subreddit.lower()
    return SUBREDDIT_INFO.get(key, (subreddit, f"ğŸ“ r/{subreddit}"))[1]


def translate_keywords(text: str) -> str:
    """Add Chinese translations for key AI terms."""
    hints = []
    text_lower = text.lower()
    for en, zh in TERM_TRANSLATIONS.items():
        if en.lower() in text_lower and len(hints) < 4:
            hints.append(f"{en}={zh}")
    return f"ğŸ’¡ {', '.join(hints)}" if hints else ""


def clean_html(raw_html: str) -> str:
    """Remove HTML tags and clean text."""
    clean = re.sub(r'<[^>]+>', '', raw_html)
    clean = html.unescape(clean)
    clean = re.sub(r'\s+', ' ', clean).strip()
    return clean


def parse_rss(xml_content: str, subreddit: str) -> List[Dict]:
    """Parse RSS feed XML into post list."""
    posts = []
    try:
        root = ET.fromstring(xml_content)
        ns = {'atom': 'http://www.w3.org/2005/Atom'}
        for entry in root.findall('atom:entry', ns):
            title = entry.find('atom:title', ns)
            link = entry.find('atom:link', ns)
            content = entry.find('atom:content', ns)
            published = entry.find('atom:published', ns)
            author = entry.find('atom:author/atom:name', ns)
            
            title_text = title.text if title is not None else "æ— æ ‡é¢˜"
            url = link.get('href', '') if link is not None else ""
            content_text = clean_html(content.text or "") if content is not None else ""
            
            # Skip pinned/announcement posts
            if "Announcing" in title_text and "discord" in url.lower():
                continue
                
            posts.append({
                "title": title_text,
                "url": url,
                "content": content_text[:300],
                "published": published.text if published is not None else "",
                "author": (author.text or "").replace("/u/", ""),
                "subreddit": subreddit,
            })
    except ET.ParseError as e:
        print(f"[Error] XML parse error: {e}", file=sys.stderr)
    return posts


def fetch_subreddit_rss(subreddit: str, sort: str = "hot", limit: int = 10) -> List[Dict]:
    """Fetch posts from subreddit RSS feed."""
    url = f"https://www.reddit.com/r/{subreddit}/{sort}.rss?limit={limit}"
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Accept": "application/rss+xml, application/xml, text/xml, */*",
        "Accept-Language": "en-US,en;q=0.9",
    }
    
    try:
        req = urllib.request.Request(url, headers=headers)
        with urllib.request.urlopen(req, timeout=15) as response:
            return parse_rss(response.read().decode("utf-8", errors="ignore"), subreddit)
    except Exception as e:
        print(f"[Error] Failed to fetch r/{subreddit}: {e}", file=sys.stderr)
        return []


def format_time_ago(iso_time: str) -> str:
    """Convert ISO time to relative time in Chinese."""
    try:
        dt = datetime.fromisoformat(iso_time.replace('+00:00', '+0000').replace('Z', '+0000'))
        now = datetime.now(dt.tzinfo)
        diff = now - dt
        hours = diff.total_seconds() / 3600
        if hours < 1:
            return f"{int(diff.total_seconds() / 60)}åˆ†é’Ÿå‰"
        elif hours < 24:
            return f"{int(hours)}å°æ—¶å‰"
        return f"{int(hours / 24)}å¤©å‰"
    except:
        return ""


def format_output(posts: List[Dict]) -> str:
    """Format posts with Chinese summaries. Limited to 3000 chars for Telegram."""
    if not posts:
        return "âŒ æœªæ‰¾åˆ°ç›¸å…³å¸–å­"
    
    lines = []
    for i, post in enumerate(posts, 1):
        title = post.get("title", "æ— æ ‡é¢˜")
        url = post.get("url", "")
        content = post.get("content", "")
        subreddit = post.get("subreddit", "")
        published = post.get("published", "")
        
        sub_desc = get_subreddit_desc(subreddit)
        keywords = translate_keywords(title + " " + content)
        time_ago = format_time_ago(published)
        zh_title = translate_title(title)
        zh_summary = summarize_content(title, content)
        
        lines.append(f"**ã€{i}ã€‘{title[:50]}**")
        lines.append(f"â€¢ ğŸ“Œ {zh_title}")
        if keywords:
            lines.append(f"â€¢ {keywords}")
        time_str = f" Â· {time_ago}" if time_ago else ""
        lines.append(f"â€¢ {sub_desc}{time_str}")
        short_url = url.replace("https://www.reddit.com", "https://reddit.com")
        lines.append(f"â€¢ ğŸ”— {short_url}\n")
    
    # é™åˆ¶æ€»é•¿åº¦
    result = "\n".join(lines)
    if len(result) > 3000:
        result = result[:3000] + "\n...ï¼ˆå†…å®¹è¿‡é•¿å·²æˆªæ–­ï¼‰"
    
    return f"ğŸ“Š **Reddit AI ç¤¾åŒºçƒ­å¸–** (å…± {len(posts)} æ¡)\n\n{result}"


def main():
    parser = argparse.ArgumentParser(description="Fetch Reddit AI posts via RSS")
    parser.add_argument("--sort", choices=["hot", "new", "top", "rising"], default="hot")
    parser.add_argument("--limit", type=int, default=3, help="Posts per subreddit")
    parser.add_argument("--subreddits", type=str, default=None, help="Comma-separated list")
    parser.add_argument("--total", type=int, default=10, help="Max total posts")
    args = parser.parse_args()
    
    subreddits = [s.strip() for s in args.subreddits.split(",")] if args.subreddits else DEFAULT_SUBREDDITS
    all_posts = []
    
    print(f"ğŸ” æŠ“å– Reddit AI ç¤¾åŒº ({args.sort})...\n", file=sys.stderr)
    for sub in subreddits:
        print(f"   ğŸ“¡ r/{sub}...", file=sys.stderr, end=" ", flush=True)
        posts = fetch_subreddit_rss(sub, args.sort, args.limit)
        print(f"âœ“ {len(posts)} ç¯‡", file=sys.stderr)
        all_posts.extend(posts)
        time.sleep(0.3)
    
    all_posts = all_posts[:args.total]
    print(f"\nğŸ“Š å…±è·å– {len(all_posts)} ç¯‡å¸–å­\n", file=sys.stderr)
    print(format_output(all_posts))


if __name__ == "__main__":
    main()
