#!/usr/bin/env python3
"""
Memory Consolidator - å®šæ—¶å¢é‡åˆå¹¶ Claude Memory æ–‡ä»¶
æ”¯æŒ MiniMax APIï¼ˆå…¼å®¹ Anthropic æ ¼å¼ï¼‰
"""

import argparse
import hashlib
import json
import logging
import os
import sys
import time
import glob
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional

import yaml
from dotenv import load_dotenv


class MemoryConsolidator:
    """è®°å¿†ç²¾ç®€åˆå¹¶å™¨"""

    def __init__(self, config_path: str = None):
        self.config = self._load_config(config_path)
        
        # åŠ è½½ .env æ–‡ä»¶ï¼ˆæ”¯æŒ cron è¿è¡Œæ—¶ï¼‰
        env_file = os.path.expanduser(self.config.get('env_file', '~/.claude/.env'))
        if os.path.exists(env_file):
            load_dotenv(env_file)
        
        self.setup_logging()
        self.setup_dirs()

    def __init__(self, config_path: str = None):
        self.config = self._load_config(config_path)
        self.setup_logging()
        self.setup_dirs()

    def _load_config(self, config_path: str = None) -> dict:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        if config_path is None:
            config_path = os.path.join(
                os.path.dirname(__file__), "..", "references", "config.json"
            )

        with open(os.path.expanduser(config_path), 'r') as f:
            return json.load(f)

    def setup_logging(self):
        """é…ç½®æ—¥å¿—"""
        log_dir = Path(os.path.expanduser(self.config.get('log_dir', '~/.openclaw/qmd_memory/logs')))
        log_dir.mkdir(parents=True, exist_ok=True)

        log_file = log_dir / f"consolidator_{datetime.now().strftime('%Y%m%d')}.log"
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_file),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)

    def setup_dirs(self):
        """åˆ›å»ºå¿…è¦ç›®å½•"""
        self.output_dir = Path(os.path.expanduser(self.config['output_dir']))
        self.output_dir.mkdir(parents=True, exist_ok=True)

        (self.output_dir / 'sources').mkdir(exist_ok=True)
        (self.output_dir / 'logs').mkdir(exist_ok=True)

    def calculate_file_hash(self, filepath: str) -> str:
        """è®¡ç®—æ–‡ä»¶ hash"""
        with open(filepath, 'rb') as f:
            return hashlib.md5(f.read()).hexdigest()

    def get_source_files(self) -> List[Dict]:
        """è·å–æºæ–‡ä»¶åˆ—è¡¨ï¼ˆåªä¿ç•™æœ€è¿‘3å¤©ï¼Œå»é‡ï¼‰"""
        import datetime
        sources = []
        seen_paths = set()
        three_days_ago = datetime.datetime.now() - datetime.timedelta(days=3)
        core_files = ['MEMORY.md', 'CLAUDE.md', 'identity.md', 'bot-ops.md', 'dev-pipeline.md', 'limits.md']
        
        for pattern in self.config['sources']:
            expanded = os.path.expanduser(pattern)
            
            # ä½¿ç”¨ glob å¤„ç†é€šé…ç¬¦
            if '*' in expanded:
                matched = glob.glob(expanded)
                for filepath in matched:
                    if os.path.isfile(filepath) and filepath not in seen_paths:
                        mtime = os.path.getmtime(filepath)
                        file_date = datetime.datetime.fromtimestamp(mtime)
                        # åªä¿ç•™æœ€è¿‘3å¤©çš„æ–‡ä»¶
                        if file_date >= three_days_ago:
                            sources.append({
                                'path': filepath,
                                'hash': self.calculate_file_hash(filepath),
                                'modified': mtime
                            })
                            seen_paths.add(filepath)
            elif os.path.exists(expanded) and expanded not in seen_paths:
                mtime = os.path.getmtime(expanded)
                file_date = datetime.datetime.fromtimestamp(mtime)
                # æ ¸å¿ƒæ–‡ä»¶ä¸å—æ—¶é—´é™åˆ¶
                if file_date >= three_days_ago or any(x in expanded for x in core_files):
                    sources.append({
                        'path': expanded,
                        'hash': self.calculate_file_hash(expanded),
                        'modified': mtime
                    })
                    seen_paths.add(expanded)
        return sources

    def check_for_changes(self, sources: List[Dict]) -> bool:
        """æ£€æŸ¥æºæ–‡ä»¶æ˜¯å¦æœ‰å˜åŒ–"""
        state_file = self.output_dir / '.source_state.json'

        if not state_file.exists():
            return True

        with open(state_file, 'r') as f:
            old_state = json.load(f)

        for source in sources:
            path = os.path.abspath(source['path'])
            if path in old_state:
                if old_state[path]['hash'] != source['hash']:
                    self.logger.info(f"æ£€æµ‹åˆ°æ–‡ä»¶å˜åŒ–: {path}")
                    return True
            else:
                self.logger.info(f"å‘ç°æ–°æ–‡ä»¶: {path}")
                return True

        return False

    def save_source_state(self, sources: List[Dict]):
        """ä¿å­˜æºæ–‡ä»¶çŠ¶æ€"""
        state_file = self.output_dir / '.source_state.json'
        state = {}
        for source in sources:
            path = os.path.abspath(source['path'])
            state[path] = {
                'hash': source['hash'],
                'modified': source['modified']
            }
        with open(state_file, 'w') as f:
            json.dump(state, f, indent=2)

    def read_source_files(self, sources: List[Dict]) -> Dict[str, str]:
        """è¯»å–æºæ–‡ä»¶å†…å®¹"""
        contents = {}
        for source in sources:
            try:
                with open(source['path'], 'r', encoding='utf-8') as f:
                    contents[source['path']] = f.read()
                # ä¿å­˜å¿«ç…§
                snapshot_path = self.output_dir / 'sources' / os.path.basename(source['path'])
                with open(snapshot_path, 'w', encoding='utf-8') as f:
                    f.write(contents[source['path']])
            except Exception as e:
                self.logger.error(f"è¯»å–æ–‡ä»¶å¤±è´¥ {source['path']}: {e}")
        return contents

    def consolidate_with_llm(self, contents: Dict[str, str]) -> str:
        """è°ƒç”¨ LLM API è¿›è¡Œç²¾ç®€ï¼ˆæ”¯æŒ MiniMax å…¼å®¹ Anthropic æ ¼å¼ï¼‰"""
        
        # é‡æ–°åŠ è½½ .envï¼ˆç¡®ä¿ cron ç¯å¢ƒä¸‹èƒ½è·å–åˆ°ï¼‰
        from dotenv import load_dotenv
        env_file = os.path.expanduser(self.config.get('env_file', '~/.claude/.env'))
        if os.path.exists(env_file):
            load_dotenv(env_file, override=True)
        
        api_key = os.environ.get(self.config.get('api_key_env', 'ANTHROPIC_AUTH_TOKEN'))
        api_base = self.config.get('api_base_url', 'https://api.minimaxi.com/anthropic')
        model = self.config.get('model', 'minimax/MiniMax-M2.1')

        if not api_key:
            self.logger.warning("æœªæ‰¾åˆ° API Keyï¼Œè·³è¿‡æ™ºèƒ½ç²¾ç®€")
            return self._simple_merge(contents)

        try:
            import httpx
            client = httpx.Client(timeout=300.0)

            prompt = self._build_consolidation_prompt(contents)

            response = client.post(
                f"{api_base}/v1/messages",
                headers={
                    "Authorization": f"Bearer {api_key}",
                    "Content-Type": "application/json",
                    "X-API-Version": "1"
                },
                json={
                    "model": model,
                    "max_tokens": 8192,
                    "messages": [{"role": "user", "content": prompt}]
                }
            )

            response.raise_for_status()
            result = response.json()
            
            self.logger.debug(f"API å“åº”: {str(result)[:500]}")

            # å…¼å®¹ä¸åŒ API å“åº”æ ¼å¼
            # 1. MiniMax æ ¼å¼: content[].text (æœ€ç»ˆç­”æ¡ˆ)
            if "content" in result and isinstance(result["content"], list) and len(result["content"]) > 0:
                for content_item in result["content"]:
                    if isinstance(content_item, dict):
                        # ä¼˜å…ˆæ‰¾ text ç±»å‹ï¼ˆæœ€ç»ˆç­”æ¡ˆï¼‰ï¼Œå…¶æ¬¡æ‰æ˜¯ thinking
                        if content_item.get("type") == "text":
                            return content_item.get("text", str(result))
                        # å¤‡ç”¨ï¼šåªæœ‰ thinking æ²¡æœ‰ text
                        if "thinking" in content_item and len(result["content"]) == 1:
                            return content_item["thinking"]
            
            # 2. OpenAI å…¼å®¹æ ¼å¼: choices[0].message.content
            if "choices" in result and isinstance(result["choices"], list) and len(result["choices"]) > 0:
                choice = result["choices"][0]
                if isinstance(choice, dict) and "message" in choice:
                    return choice["message"].get("content", str(result))
            
            # 3. ç›´æ¥è¿”å›ç»“æœå­—ç¬¦ä¸²
            return str(result)

        except ImportError:
            self.logger.warning("httpx æœªå®‰è£…ï¼Œå°è¯•ä½¿ç”¨ anthropic SDK")
            return self._use_anthropic_sdk(contents)
        except Exception as e:
            self.logger.error(f"API è°ƒç”¨å¤±è´¥: {e}")
            return self._simple_merge(contents)

    def _use_anthropic_sdk(self, contents: Dict[str, str]) -> str:
        """å¤‡ç”¨ï¼šä½¿ç”¨ anthropic SDK"""
        try:
            from anthropic import Anthropic
            api_key = os.environ.get('ANTHROPIC_AUTH_TOKEN', '')
            client = Anthropic(api_key=api_key, base_url='https://api.minimaxi.com/anthropic')

            prompt = self._build_consolidation_prompt(contents)

            response = client.messages.create(
                model='minimax/MiniMax-M2.1',
                max_tokens=8192,
                messages=[{"role": "user", "content": prompt}]
            )

            return response.content[0].text
        except Exception as e:
            self.logger.error(f"Anthropic SDK ä¹Ÿå¤±è´¥: {e}")
            return self._simple_merge(contents)

    def _build_consolidation_prompt(self, contents: Dict[str, str]) -> str:
        """æ„å»ºç²¾ç®€æç¤º"""
        combined = ""
        for path, content in contents.items():
            combined += f"\n\n=== {path} ===\n{content[:10000]}"

        return f"""
è¯·å°†ä»¥ä¸‹å¤šä¸ªè®°å¿†æ–‡ä»¶åˆå¹¶ç²¾ç®€ï¼Œå»é™¤é‡å¤ï¼Œä¿ç•™æ ¸å¿ƒä¿¡æ¯ï¼š

{combined}

**æ ¸å¿ƒåŸåˆ™ï¼šåˆå¹¶å¤šä¸ªè®°å¿†æ–‡ä»¶å¹¶å»é‡ï¼Œä¿ç•™æ‰€æœ‰æœ‰ä»·å€¼çš„å†…å®¹**

è¯·è¾“å‡ºï¼ŒåŠ¡å¿…ä¸å½±å“è´¨é‡ï¼š
1. æ ¸å¿ƒè§„åˆ™å’ŒåŸåˆ™
2. ç”¨æˆ·åå¥½å’Œé‡è¦çº¦å®š
3. å¾…åŠäº‹é¡¹
4. é‡è¦æ•™è®­å’Œç»éªŒ
5. é¡¹ç›®ç´¢å¼•å’ŒçŸ¥è¯†åº“
6. å…³é”®æ­¥éª¤ã€é€»è¾‘ã€æŠ€å·§ç­‰

æ ¼å¼ä¸ºæ¸…æ™°çš„ Markdownï¼Œä¿æŒç»“æ„å’Œå¯è¯»æ€§ã€‚
"""

    def _simple_merge(self, contents: Dict[str, str]) -> str:
        """ç®€å•åˆå¹¶ï¼ˆæ—  APIï¼‰"""
        merged = []
        for path, content in contents.items():
            merged.append(f"\n\n=== {path} ===\n{content}")
        return "\n".join(merged)

    def deduplicate(self, content: str) -> str:
        """å»é‡å¤„ç†"""
        # ç®€åŒ–ç‰ˆï¼šæŒ‰è¡Œå»é‡
        lines = content.split('\n')
        seen = set()
        result = []

        for line in lines:
            line_stripped = line.strip()
            if line_stripped and line_stripped not in seen:
                seen.add(line_stripped)
                result.append(line)

        return '\n'.join(result)

    def save_output(self, content: str):
        """ä¿å­˜è¾“å‡ºæ–‡ä»¶"""
        output_file = self.output_dir / 'consolidated.md'

        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(content)

        # è‡ªåŠ¨æå–çƒ­è®°å¿†ï¼ˆå‰50è¡Œï¼‰
        self._extract_hot_memory(content)

        # ä¿å­˜å…ƒæ•°æ®
        meta = {
            'last_run': datetime.now().isoformat(),
            'files_processed': len(list((self.output_dir / 'sources').glob('*'))),
            'output_size': len(content)
        }
        with open(self.output_dir / '.last_run', 'w') as f:
            f.write(json.dumps(meta, indent=2))

        self.logger.info(f"è¾“å‡ºå·²ä¿å­˜: {output_file}")

    def _extract_hot_memory(self, content: str):
        """ä» consolidated.md è‡ªåŠ¨æå–å‰ 50 è¡Œä½œä¸ºçƒ­è®°å¿†"""
        lines = content.split('\n')[:50]
        hot_content = '\n'.join(lines)

        # ä¿å­˜åˆ° ~/.openclaw/qmd_memory/hot.mdï¼ˆçƒ­è®°å¿†ï¼‰
        hot_file = self.output_dir / 'hot.md'
        with open(hot_file, 'w', encoding='utf-8') as f:
            f.write(hot_content)

        self.logger.info(f"çƒ­è®°å¿†å·²æ›´æ–°: {hot_file}")

    def run(self):
        """æ‰§è¡Œå®Œæ•´æµç¨‹"""
        self.logger.info("=== å¼€å§‹è®°å¿†ç²¾ç®€ ===")

        # 1. è·å–æºæ–‡ä»¶
        sources = self.get_source_files()
        self.logger.info(f"å‘ç° {len(sources)} ä¸ªæºæ–‡ä»¶")

        if not sources:
            self.logger.info("æ— æºæ–‡ä»¶ï¼Œé€€å‡º")
            return

        # 2. æ£€æŸ¥å˜åŒ–
        if not self.check_for_changes(sources):
            self.logger.info("æ— æ–‡ä»¶å˜åŒ–ï¼Œè·³è¿‡å¤„ç†")
            return

        # 3. è¯»å–å†…å®¹
        contents = self.read_source_files(sources)
        self.logger.info(f"è¯»å–äº† {len(contents)} ä¸ªæ–‡ä»¶")

        # 3.5 ç´¯ç§¯æ¨¡å¼ï¼šå°†æ–°å†…å®¹è¿½åŠ åˆ°ç°æœ‰çš„ consolidated.mdï¼ˆä¸è¦†ç›–ï¼‰
        existing_consolidated = self.output_dir / 'consolidated.md'
        if existing_consolidated.exists():
            try:
                with open(existing_consolidated, 'r', encoding='utf-8') as f:
                    existing_content = f.read()
                # è¿½åŠ åˆ°æºæ–‡ä»¶æœ«å°¾ï¼Œä½œä¸ºé¢å¤–å‚è€ƒ
                contents['[ç´¯ç§¯] ç°æœ‰ consolidated.md'] = existing_content
                self.logger.info("å·²åŠ è½½ç°æœ‰ consolidated.mdï¼ˆç´¯ç§¯æ¨¡å¼ï¼šåªå¢ä¸å‡ï¼‰")
            except Exception as e:
                self.logger.warning(f"è¯»å–ç°æœ‰ consolidated.md å¤±è´¥: {e}")

        # 4. LLM ç²¾ç®€ï¼ˆMiniMaxï¼‰
        consolidated = self.consolidate_with_llm(contents)
        self.logger.info("å®Œæˆæ™ºèƒ½ç²¾ç®€")

        # 5. å»é‡
        consolidated = self.deduplicate(consolidated)
        self.logger.info("å®Œæˆå»é‡")

        # 6. ä¿å­˜
        self.save_output(consolidated)

        # 7. ä¿å­˜çŠ¶æ€
        self.save_source_state(sources)

        self.logger.info("=== è®°å¿†ç²¾ç®€å®Œæˆ ===")


def main():
    parser = argparse.ArgumentParser(description='Memory Consolidator')
    parser.add_argument('--run-now', action='store_true', help='ç«‹å³æ‰§è¡Œ')
    parser.add_argument('--config', type=str, help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--verbose', '-v', action='store_true', help='è¯¦ç»†è¾“å‡º')

    args = parser.parse_args()

    consolidator = MemoryConsolidator(args.config)

    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)

    consolidator.run()
    
    # å‘é€å®Œæˆé€šçŸ¥åˆ° feed
    notify_feed(f"Memory Consolidator å®Œæˆï¼Œå¢é‡åˆå¹¶ {consolidatorå¢é‡å†…å®¹}")


def notify_feed(message: str):
    """å‘é€é€šçŸ¥åˆ° feed topic"""
    import subprocess
    try:
        subprocess.run(
            ["/usr/bin/openclaw", "message", "send",
             "--channel", "telegram",
             "--target", "-1003856805564",
             "--thread-id", "1816",
             "--message", f"ğŸ“ {message}"],
            capture_output=True, timeout=10
        )
    except Exception:
        pass


if __name__ == '__main__':
    main()
